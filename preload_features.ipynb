{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Get database connection string from environment variable\n",
    "DB_CONNECTION_STRING = os.getenv(\"DB_CONNECTION_STRING\")\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Создаем соединение с базой данных\n",
    "engine = create_engine(DB_CONNECTION_STRING)\n",
    "\n",
    "# Загружаем данные из таблиц user_data, post_text_df и feed_data\n",
    "user_data = pd.read_sql(\"SELECT * FROM public.user_data\", con=engine)\n",
    "post_text_df = pd.read_sql(\"SELECT * FROM public.post_text_df\", con=engine)\n",
    "feed_data = pd.read_sql(\"SELECT * FROM public.feed_data ORDER BY timestamp DESC LIMIT 5000000\", con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import RobertaModel\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)  # Move the model to the GPU\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove extra whitespaces\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove leading and trailing whitespaces\n",
    "    text = text.strip()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove special characters except punctuation\n",
    "    text = re.sub(r'[^a-zA-Z0-9.,!?\\'\"\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def get_embeddings(text, model, tokenizer):\n",
    "    \n",
    "    encoding = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=False,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "    encoding = {k: v.to(device) for k, v in encoding.items()}  # Move input tensors to the GPU\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoding)\n",
    "    return output.pooler_output.squeeze().detach().cpu().numpy()\n",
    "\n",
    "embeddings = []\n",
    "for text in tqdm(post_text_df['text']):\n",
    "    if text.strip():  # Skip empty strings\n",
    "        preprocessed_text = preprocess_text(text)\n",
    "        embedding = get_embeddings(preprocessed_text, model, tokenizer)\n",
    "        embeddings.append(embedding)\n",
    "\n",
    "# Преобразование списка эмбеддингов в массив numpy\n",
    "embeddings = np.array(embeddings)\n",
    "\n",
    "# Создание отдельных столбцов для каждого эмбеддинга\n",
    "embedding_columns = {}\n",
    "for i in range(embeddings.shape[1]):\n",
    "    column_name = f'embedding_{i}'\n",
    "    embedding_columns[column_name] = embeddings[:, i]\n",
    "\n",
    "# Создание нового датафрейма с отдельными столбцами для эмбеддингов\n",
    "embedding_df = pd.DataFrame(embedding_columns)\n",
    "\n",
    "# Объединение датафрейма эмбеддингов с другими признаками\n",
    "merged_df = pd.concat([post_text_df, embedding_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объединяем данные в один DataFrame\n",
    "user_feed = pd.merge(feed_data, user_data, on='user_id', how='left')\n",
    "all_data = pd.merge(user_feed, merged_df, on='post_id', how='left')\n",
    "\n",
    "def generate_new_target(target, action):\n",
    "    if target == 1 or action == 'like':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "all_data['target'] = all_data.apply(lambda row: generate_new_target(row['target'], row['action']), axis=1)\n",
    "\n",
    "# Создаем признаки\n",
    "all_data['timestamp'] = pd.to_datetime(all_data['timestamp'])\n",
    "all_data['day_of_week'] = all_data['timestamp'].dt.dayofweek\n",
    "all_data['hour'] = all_data['timestamp'].dt.hour\n",
    "all_data['time_slot'] = all_data['day_of_week'].astype(str) + '_' + all_data['hour'].astype(str)\n",
    "\n",
    "train = all_data.drop(['action', 'timestamp'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "205"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate User Features\n",
    "# Add user mean to train\n",
    "user_means = train.groupby('user_id')['target'].mean()\n",
    "train['user_means'] = train['user_id'].map(user_means)\n",
    "# Target by'topic'\n",
    "unique_topics = train['topic'].unique()\n",
    "target_column_names = []\n",
    "for topic in unique_topics:\n",
    "    topic_col_name = f\"{topic}_target\"\n",
    "    target_column_names.append(topic_col_name)\n",
    "    user_target_by_topic = train[train['target'] == 1].groupby(['user_id', 'topic']).size().reset_index(name='temp')\n",
    "    user_target_by_topic = user_target_by_topic[user_target_by_topic['topic'] == topic].rename(columns={'temp': topic_col_name})\n",
    "    user_target_by_topic = user_target_by_topic[['user_id', topic_col_name]]\n",
    "    train = pd.merge(train, user_target_by_topic, on='user_id', how='left')\n",
    "    train[topic_col_name].fillna(0, inplace=True)\n",
    "    \n",
    "# Generate user table and preload to db\n",
    "user_columns = ['user_id', 'user_means'] + target_column_names\n",
    "\n",
    "user_features = train[user_columns].drop_duplicates(subset=['user_id'])\n",
    "user_data_enriched = pd.merge(user_data, user_features, on='user_id', how='left')\n",
    "\n",
    "for col in user_columns:\n",
    "       user_data_enriched[col] = user_data_enriched[col].fillna(user_data_enriched[col].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total likes for each post\n",
    "total_likes = train.groupby('post_id')['target'].sum().reset_index()\n",
    "total_likes.columns = ['post_id', 'total_likes']\n",
    "train = train.merge(total_likes, on='post_id', how='left')\n",
    "# Calculate post CTR (Click-Through Rate)\n",
    "post_ctr = train.groupby('post_id')['target'].mean().reset_index()\n",
    "post_ctr.columns = ['post_id', 'post_ctr']\n",
    "train = train.merge(post_ctr, on='post_id', how='left')\n",
    "\n",
    "# Prepare post features DataFrame for loading to DB\n",
    "post_features_columns = ['post_id'] + ['total_likes', 'post_ctr']\n",
    "post_features = train[post_features_columns].drop_duplicates(subset=['post_id'])\n",
    "post_data_enriched = pd.merge(post_text_df, post_features, on='post_id', how='left')\n",
    "\n",
    "for col in post_features_columns:\n",
    "       post_data_enriched[col] = post_data_enriched[col].fillna(post_data_enriched[col].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_data_enriched.to_sql('martynov_post_features_lesson_22_posts', con=engine, if_exists='replace', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
